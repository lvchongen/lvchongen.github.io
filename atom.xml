<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>网络小虫成长记</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lvchongen.github.io/"/>
  <updated>2019-03-29T05:23:52.860Z</updated>
  <id>https://lvchongen.github.io/</id>
  
  <author>
    <name>Chongen Lv</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>vPC</title>
    <link href="https://lvchongen.github.io/2019/03/29/vPC/"/>
    <id>https://lvchongen.github.io/2019/03/29/vPC/</id>
    <published>2019-03-29T02:51:45.000Z</published>
    <updated>2019-03-29T05:23:52.860Z</updated>
    
    <content type="html"><![CDATA[<h1 id="vPC"><a href="#vPC" class="headerlink" title="vPC"></a>vPC</h1><p>vPC(Virtural Port-Channel) 是Cisco Nexus系列交换机中的一个特性。它支持一个跨机箱的二层Port-Channel. 对于第三方设备来说(交换机或服务器) 物理上连接到了两台交换机，但逻辑上是一台交换机。</p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/vPC_tuopu.png" alt="vPC_tuopu"></p><p>vPS的优势：</p><ul><li>消除STP阻塞端口.<ul><li>STP:  生成协议树， 逻辑上断开环路，防止二层网络的广播风暴产生。</li></ul></li><li>使用所有可用的链路带宽</li><li>允许服务器双主上连</li><li>当链路或设备失效后提供了快速的收敛</li><li>为服务器提供了双active的网关</li></ul><p>vPS术语：</p><ul><li>vPC Peer: 启用vPC的交换机被称为vPC的Peer。</li><li>vPC domain： 两台启用vPC的交换机所有的逻辑域。</li><li>vPC Peer Keepalive Link : Peer Keepalive link 是一个三层链路，用于在peer-link 失效后检测vPC Peer是否存活。</li><li>vPC Peer Link : 用于传输vPC控制层面的流量，包括同步mac地址表，vPC成员信息，IGMP信息。<ul><li>IGMP:  Internet组管理协议称为IGMP协议(Internet Group Management Protocol) ， 是因特网协议家族中的一个组播协议。该协议运行在主机和主播路由器之间。 </li></ul></li><li>Orphan Port: 孤立端口，没有加入vPC的端口</li><li>vPC Member Port: 加入vPC的端口，是一个Port-Channel接口。<ul><li>Port-Channel: port group 是配置层面上的一个物理端口组，配置到port group里面的物理端口才可以参加链路汇聚，并成为port channel里的某个成员端口。在逻辑上，port group 并不是一个端口，而是一个端口序列。加入port group 中的物理端口满足某种条件时进行端口汇聚，形成一个port channel，这个port channel 具备了逻辑端口的属性，才真正成为一个独立的逻辑端口。</li></ul></li></ul><p>vPC 防环：</p><p>vPC执行一个数据层面的防环来代替控制层面的STP。vPC peer-link被使用在两台vPC设备之间同步mac地址、vPC成员状态信息和IGMP。从vPC member port进来的流量，穿越vpc peer-link之后，不会再被允许从任何vPC Member port发出，但可以从其实接口（L3 Port或孤立端口转发）。vPC基于这样的机制来进行防环</p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/vPC_Domain.png" alt="vPC_Domain"></p><p>vPC成员端口失效</p><p>如果一个vPC member port失效，和普通的Port-Channel机制一样，另一个端口会继续转发数据。</p><p>vPC Peer Link失效</p><p>当Peer link失效以后，vpc设备会通过peer keepalive link来检测Primary交换机是否存活，如果Secondary交换机能够继续收到Primary交换机发过来的信息，表示Primary交换机未宕机，则Secondary Peer会shutdown所有vpc member port，Primary交换机继续转发流量。</p><p>vPC Primary Switch 失效</p><p>如果Primary交换机无效，Secondary交换机会变成Primary, 继续转发流量。 vPC role不支持抢占，Priority越小越优先。</p><p>vPC Peer keepalive link和 Peer link同时失效</p><p>当Peer keepalive linke和Peer Link同时失效以后，Secondary交换机也会运行在Priamry模式下，两台交换机都会转发流量。</p><p>vPC Peer keepalive失效</p><p>只有Peer keepalive link失效并不会影响vpc的工作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;vPC&quot;&gt;&lt;a href=&quot;#vPC&quot; class=&quot;headerlink&quot; title=&quot;vPC&quot;&gt;&lt;/a&gt;vPC&lt;/h1&gt;&lt;p&gt;vPC(Virtural Port-Channel) 是Cisco Nexus系列交换机中的一个特性。它支持一个跨机箱的二层Port
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Nexus9K_SNMP</title>
    <link href="https://lvchongen.github.io/2019/03/25/Nexus9K-SNMP/"/>
    <id>https://lvchongen.github.io/2019/03/25/Nexus9K-SNMP/</id>
    <published>2019-03-25T06:56:05.000Z</published>
    <updated>2019-03-25T07:12:07.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Nexus9K-commands-SNMP"><a href="#Nexus9K-commands-SNMP" class="headerlink" title="Nexus9K_commands_SNMP"></a><strong>Nexus9K_commands_SNMP</strong></h1><h3 id="SNMP-简单概述"><a href="#SNMP-简单概述" class="headerlink" title="SNMP 简单概述"></a>SNMP 简单概述</h3><p>SNMP是英文”Simple Network Management Protocol”的缩写，中文意思是”简单网络管理协议”。SNMP是一种简单网络管理协议，它属于TCP/IP五层协议中的应用层协议，用于网络管理的协议。SNMP主要用于网络设备的管理，</p><p>SNMP协议主要由两大部分组成：SNMP管理站和SNMP代理。SNMP管理站是一个中心节点，负责收集维护各个SNMP元素的信息，并对这些信息进行处理，最后反馈给网络管理员；而SNMP代理是运行在各个被管理的网络节点上，负责统计该节点的各项信息，并且负责与SNMP管理站交互，接受并执行管理站的命令，上传各种本地的网络信息。</p><p>SNMP管理站和SNMP代理之间是松散耦合，它们之间的通信是通过UDP协议完成的。一般情况下，SNMP管理站通过UDP协议向SNMP代理发送各种命令，当SNMP代理收到命令后，返回SNMP管理站所需要的参数。但是当SNMP代理检测到网络元素异常的时候，也可以主动向SNMP管理站发送消息，通告当前异常状况。</p><p>SNMP的基本思想：为不同种类的设备、不同厂家生产的设备、不同型号的设备，定义为一个统一的接口和协议，使得管理员可以是使用统一的外观面对这些需要管理的网络设备进行管理。通过网络，管理员可以管理位于不同物理空间的设备，从而大大提高网络管理的效率，简化网络管理员的工作。</p><p>　　SNMP的工作方式：管理员需要向设备获取数据，所以SNMP提供了【读】操作；管理员需要向设备执行设置操作，所以SNMP提供了【写】操作；设备需要在重要状况改变的时候，向管理员通报事件的发生，所以SNMP提供了【Trap】操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Nexus9K-commands-SNMP&quot;&gt;&lt;a href=&quot;#Nexus9K-commands-SNMP&quot; class=&quot;headerlink&quot; title=&quot;Nexus9K_commands_SNMP&quot;&gt;&lt;/a&gt;&lt;strong&gt;Nexus9K_command
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spine-Leaf 架构设计综述</title>
    <link href="https://lvchongen.github.io/2019/03/14/Spine-Leaf/"/>
    <id>https://lvchongen.github.io/2019/03/14/Spine-Leaf/</id>
    <published>2019-03-14T05:32:05.000Z</published>
    <updated>2019-03-14T08:33:33.050Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据中心网络：Spine-Leaf-架构设计综述"><a href="#数据中心网络：Spine-Leaf-架构设计综述" class="headerlink" title="数据中心网络：Spine-Leaf 架构设计综述"></a>数据中心网络：Spine-Leaf 架构设计综述</h1><h3 id="数据中心演进"><a href="#数据中心演进" class="headerlink" title="数据中心演进"></a>数据中心演进</h3><p>The architecture consists of core routers, aggregation routers (sometimes called distribution routers), and access switches. Between the aggregation routers and access switches, Spanning Tree Protocol is used to build a loopfree topology for the Layer 2 part of network. Spanning Tree Protocol provides several benefits: it is simple, and it is a plug-and-play technology requiring little configuration. VLANs are extended within each pod that servers can move freely within the pod without the need to change IP address and default gateway configurations. However, Spanning Tree Protocol cannot use parallel forwarding paths, and it always blocks redundant paths in a VLAN. </p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/Old.png" alt="Old_DataCenter"></p><ul><li><strong>核心路由器(Core Routers)</strong>： 核心路由器又称”骨干路由器”，是位于网络中心的路由器。</li><li><strong>路由聚合(Aggregation Routers)</strong>： 路由汇聚的“含义”是把一组路由汇聚为一个单个的路由。路由汇聚的最终结果和最明显的好处是缩小网络上的路由表的尺寸。这样将减少与每一个路由跳有关的延迟，因为由于减少了路由登录项数量，查询路由表的平均时间将加快。路由汇聚的“用意”是当我们采用了一种体系化编址规划后的一种用一个IP地址代表一组IP地址的集合的方法。</li><li><strong>接入层交换机(Access Switches)</strong>: 通常将网络中直接面向用户连接或访问网络的部分称为接入层，将位于接入层和核心层之间的部分称为分布层或汇聚层。接入交换机一般用于直接连接电脑，汇聚交换机一般用于楼宇间。汇聚相当于一个局部或重要的中转站，核心相当于一个出口或总汇总。原来定义的汇聚层的目的是为了减少核心的负担，将本地数据交换机流量在本地的汇聚交换机上交换，减少核心层的工作负担，使核心层只处理到本地区域外的数据交换。</li><li><strong>生成树协议(Spanning Tree Protocol)</strong>: 一种工作在OSI网络模型中的第二层数据链路层的通信协议，基本应用是防止交换机冗余链路产生的环路.用于确保以太网中无环路的逻辑拓扑结构.从而避免了广播风暴,大量占用交换机的资源。</li><li><strong>VLAN</strong>: VLAN虚拟局域网是对连接到的第二层交换机端口的网络用户的逻辑分段，不受网络用户的物理位置限制而根据用户需求进行网络分段。一个VLAN可以在一个交换机或者跨交换机实现。VLAN可以根据网络用户的位置、作用、部门或者根据网络用户所使用的应用程序和协议来进行分组。基于交换机的虚拟局域网能够为局域网解决冲突域、广播域、带宽问题。</li></ul><p>In 2010, Cisco introduced virtual-port-channel (vPC) technology to overcome the limitations of Spanning Tree Protocol. vPC eliminates the spanning-tree blocked ports, provides active-active uplink from the access switches to the aggregation routers, and makes full use of the available bandwidth, as shown in Figure 2. With vPC technology, Spanning Tree Protocol is still used as a fail-safe mechanism. vPC technology works well in a relatively small data center environment in which most traffic consists of northbound and southbound communication between clients and servers.</p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/Second.png" alt="Second"></p><ul><li><strong>vPC</strong>: vPC 解放了被 STP 禁用的端口，提供接入交换机到汇聚路由器之间的 active-active 上行链路， 充分利用可用的带宽</li></ul><p>Since 2003, with the introduction of virtual technology, the computing, networking, and storage resources that were segregated in pods in Layer 2 in the three-tier data center design can be pooled. This revolutionary technology created a need for a larger Layer 2 domain, from the access layer to the core layer, as shown in Figure 3.</p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/Thired.png" alt="Third"></p><p>With virtualized servers, applications are increasingly deployed in a distributed fashion, which leads to increased east-west traffic. This traffic needs to be handled efficiently, with low and predictable latency. However, vPC can provide only two active parallel uplinks, and so bandwidth becomes a bottleneck in a three-tier data center architecture. Another challenge in a three-tier architecture is that server-to-server latency varies depending on the traffic path used. A new data center design called the Clos network–based spine-and-leaf architecture was developed to overcome these limitations. This architecture has been proven to deliver the high-bandwidth, low-latency, nonblocking serverto-server connectivity.</p><h3 id="Spine-and-Leaf-Architecture"><a href="#Spine-and-Leaf-Architecture" class="headerlink" title="Spine-and-Leaf Architecture"></a>Spine-and-Leaf Architecture</h3><p>In this two-tier Clos architecture, every lower-tier switch (leaf layer) is connected to each of the top-tier switches (spine layer) in a full-mesh topology. The leaf layer consists of access switches that connect to devices such as servers. The spine layer is the backbone of the network and is responsible for interconnecting all leaf switches. Every leaf switch connects to every spine switch in the fabric. The path is randomly chosen so that the traffic load is evenly distributed among the top-tier switches. If one of the top tier switches were to fail, it would only slightly degrade performance throughout the data center. If oversubscription of a link occurs (that is, if more traffic is generated than can be aggregated on the active link at one time), the process for expanding capacity is straightforward. An additional spine switch can be added, and uplinks can be extended to every leaf switch, resulting in the addition of interlayer bandwidth and reduction of the oversubscription. If device port capacity becomes a concern, a new leaf switch can be added by connecting it to every spine switch and adding the network configuration to the switch. The ease of expansion optimizes the IT department’s process of scaling the network. If no oversubscription occurs between the lower-tier switches and their uplinks, then a nonblocking architecture can be achieved. With a spine-and-leaf architecture, no matter which leaf switch to which a server is connected, its traffic always has to cross the same number of devices to get to another server (unless the other server is located on the same leaf). This approach keeps latency at a predictable level because a payload only has to hop to a spine switch and another leaf switch to reach its destination.</p><p><img src="https://lvchongen-1255888772.cos.ap-chengdu.myqcloud.com/Spine.png" alt="Spine-leaf"></p><p>在以上两级 Clos 架构中，<strong>每个低层级的交换机（leaf）都会连接到每个高层级的交换机 （spine），形成一个 full-mesh 拓扑</strong>。leaf 层由接入交换机组成，用于连接服务器等 设备。spine 层是网络的骨干（backbone），负责将所有的 leaf 连接起来。 fabric 中的每个 leaf 都会连接到每个 spine，如果一个 spine 挂了，数据中心的吞吐性 能只会有轻微的下降（slightly degrade）。</p><p>如果某个链路被打满了，扩容过程也很直接：添加一个 spine 交换机就可以扩展每个 leaf 的上行链路，增大了 leaf 和 spine 之间的带宽，缓解了链路被打爆的问题。如果接入层 的端口数量成为了瓶颈，那就直接添加一个新的 leaf，然后将其连接到每个 spine 并做相 应的配置即可。这种易于扩展（ease of expansion）的特性优化了 IT 部门扩展网络的过 程。<strong>leaf 层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞</strong>（nonblocking）。</p><p><strong>在 Spine-and-Leaf 架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量 的设备（除非这两个服务器在同一 leaf 下面），这保证了延迟是可预测的</strong>，因为一个包 只需要经过一个 spine 和另一个 leaf 就可以到达目的端。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据中心网络：Spine-Leaf-架构设计综述&quot;&gt;&lt;a href=&quot;#数据中心网络：Spine-Leaf-架构设计综述&quot; class=&quot;headerlink&quot; title=&quot;数据中心网络：Spine-Leaf 架构设计综述&quot;&gt;&lt;/a&gt;数据中心网络：Spine-L
      
    
    </summary>
    
    
  </entry>
  
</feed>
